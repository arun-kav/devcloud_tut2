{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# RF Classifier Sample Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    - **rf_FP32.xml + rf_FP32.bin** - The xml file of the model (or rf_FP16.xml + rf_FP16.bin) \n",
    "    - **rf_classifier.py** - Base Python* code to perform inference\n",
    "    - **RF_TUTORIAL.ipynb** - This Jupyter* Notebook\n",
    "    - **rf_classy.sh** - Deployment shell script\n",
    "    - **RML2016.10a_dict.pkl** - The dataset on which to test on, in .pkl format\n",
    "    \n",
    "It is recommended that you have already read the following from [Get Started on the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/home/):\n",
    "- [Overview of the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/get_started/devcloud/)\n",
    "- [Overview of the Intel® Distribution of OpenVINO™ toolkit](https://devcloud.intel.com/edge/get_started/openvino/)\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.  If you download or copy to a new server, this sample may not run.</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This sample application demonstrates how inference on RF data may be performed using Intel® hardware and software tools. This solution uses a model derived from the following author: \n",
    "\n",
    "[@article{convnetmodrec,\n",
    "  title={Convolutional Radio Modulation Recognition Networks},\n",
    "  author={O'Shea, Timothy J and Corgan, Johnathan and Clancy, T. Charles},\n",
    "  journal={arXiv preprint arXiv:1602.04105},\n",
    "  year={2016}\n",
    "}](https://github.com/radioML/examples/blob/master/modulation_recognition/RML2016.10a_VTCNN2_example.ipynb)\n",
    "\n",
    "[@article{rml_datasets,\n",
    "  title={Radio Machine Learning Dataset Generation with GNU Radio},\n",
    "  author={O'Shea, Timothy J and West, Nathan},\n",
    "  journal={Proceedings of the 6th GNU Radio Conference},\n",
    "  year={2016}\n",
    "}](https://github.com/radioML/examples/blob/master/modulation_recognition/RML2016.10a_VTCNN2_example.ipynb) \n",
    "\n",
    "Additionally, the RML2016.10a dataset is used for this work (https://www.deepsig.ai/datasets).\n",
    "\n",
    "\n",
    "### Key concepts\n",
    "This sample application includes an example for the following:\n",
    "- Application:\n",
    "  - Timestream data as input is supported in OpenVINO\n",
    "  - Inference can be performed on large datasets\n",
    "  - Inference can be run in multiple batches\n",
    "- Intel® DevCloud for the Edge:\n",
    "  - Submitting inference as jobs that are performed on different edge compute nodes (rather than on the development node hosting this Jupyter* notebook)\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) and [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classification\n",
    "\n",
    "The RF modulation classifier uses the Intel® Distribution of OpenVINO™ toolkit to classify 11 different modulations of the RF data, as described by the RADIOML 2016.10A dataset. The following steps are taken to take output\n",
    "\n",
    "1. Convert the model to IR to be used for inference\n",
    "2. Create the job file used to submit running inference on compute nodes\n",
    "3. Submit jobs for different compute nodes and monitor the job status until complete\n",
    "4. View results and assess performance \n",
    "\n",
    "### How it works\n",
    "At startup the RF application configures itself by parsing the command line arguments.  Once configured, the application loads the specified inference model's IR files into the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) and runs inference on the specified timeframe data.\n",
    "\n",
    "To run the application on the Intel® DevCloud for the Edge, a job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and and Intel® Arria® 10 FPGA.  After inference on the input is completed, the output is stored in the appropriate `results/<architecture>/` directory.  The results are then viewed within this Jupyter* Notebook.\n",
    "\n",
    "The application and inference code for this sample is already implemented in the two Python* files, as described above.\n",
    "\n",
    "The following sections will guide you through configuring and running the RF modulation classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The following sections describe all the necessary configuration to run the RF classifier application.\n",
    "#### Command line arguments\n",
    "The application is run from the command line using the following format:\n",
    "```bash\n",
    "python3 rf_classifier.py <arguments...>\n",
    "```\n",
    "The required command line _<arguments...>_ to run the Python* executable [`rf_classifier.py`](./rf_classifier.py) are:\n",
    "- **-m** - Path to the _.xml_ IR file (created using [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)) for the inference model\n",
    "- **-o** - The path to where the output video file will be stored\n",
    "- **-d** - Device type to use to run inference (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "\n",
    "Note: Other examples will have a -i argument for inputs because the desired files to input will be in your file directory. The dataset we wish to test on contains 60000 datapoints, but we can't iterate through them one-by-one in just the files. We will unpack the dataset in the classification code itself, so there isn't an excessive use of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IR files for the inference model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.). \n",
    "The Intel® Distribution of OpenVINO™ toolkit also includes the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) utility  to download some common inference models from the [Open Model Zoo](https://github.com/opencv/open_model_zoo). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "The following sections will go through the steps to run our inference application on the Intel® DevCloud for the Edge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the classification script below, which imports in the dataset for testing, formats it, and performs inference using the OpenVINO™ IECore. Make sure the RML2016.10a_dict.pkl file is in the same directory, or change the path to it in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from qarpo.demoutils import *\n",
    "import applicationMetricWriter\n",
    "from time import time\n",
    "\n",
    "# Setup the dataset\n",
    "import _pickle as cPickle\n",
    "\n",
    "# Make sure this file is in your current directory!\n",
    "with open(\"RML2016.10a_dict.pkl\", 'rb') as f:\n",
    "    Xd = cPickle.load(f, encoding=\"latin1\") \n",
    "\n",
    "#Setup the input data: snr = signal to noise ratio, mod = modulation scheme\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "\n",
    "#Let X be our array of modulations\n",
    "X = []\n",
    "labels = []\n",
    "\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  labels.append((mod,snr))\n",
    "X = np.vstack(X)\n",
    "\n",
    "#Setup inference engine\n",
    "try:\n",
    "    from openvino import inference_engine as ie\n",
    "    from openvino.inference_engine import IENetwork, IECore, IEPlugin\n",
    "    \n",
    "except Exception as e:\n",
    "    exception_type = type(e).__name__\n",
    "    print(\"The following error happened while importing Python API module:\\n[ {} ] {}\".format(exception_type, e))\n",
    "    sys.exit(1)\n",
    "\n",
    "def build_argparser():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"--model\", help=\"Path to an .xml file with a trained model.\", required=True, type=str)\n",
    "    parser.add_argument(\"-d\", \"--device\",\n",
    "                        help=\"Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample \"\n",
    "                             \"will look for a suitable plugin for device specified (CPU by default)\", default=\"CPU\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"-ni\", \"--number_iter\", help=\"Number of inference iterations\", default=1, type=int)\n",
    "    parser.add_argument(\"-l\", \"--cpu_extension\",\n",
    "                        help=\"MKLDNN (CPU)-targeted custom layers.Absolute path to a shared library with the kernels \"\n",
    "                             \"impl.\", type=str, default=None)\n",
    "    parser.add_argument(\"-pp\", \"--plugin_dir\", help=\"Path to a plugin folder\", type=str, default=None)\n",
    "    parser.add_argument(\"--num_threads\", default=88, type=int)\n",
    "    parser.add_argument(\"-nt\", \"--number_top\", help=\"Number of top results\", default=11, type=int)\n",
    "    parser.add_argument(\"-p\", \"--out_dir\", help=\"Optional. The path where result files and logs will be stored\",\n",
    "                      required=False, default=\"./results\", type=str)\n",
    "    parser.add_argument(\"-o\", \"--out_prefix\", \n",
    "                      help=\"Optional. The file name prefix in the output_directory where results will be stored\", \n",
    "                      required=False, default=\"out_\", type=str)\n",
    "    parser.add_argument(\"-g\", \"--log_prefix\", \n",
    "                      help=\"Optional. The file name prefix in the output directory for log files\",\n",
    "                      required=False, default=\"log_\", type=str)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def main():\n",
    "    # Run inference\n",
    "    job_id = os.getenv(\"PBS_JOBID\")    \n",
    "    \n",
    "    # Plugin initialization for specified device and load extensions library if specified.\n",
    "    args = build_argparser().parse_args()\n",
    "    model_xml = args.model\n",
    "    model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "\n",
    "    # Set up logging to a file\n",
    "    logpath = os.path.join(os.path.dirname(__file__), \".log\")\n",
    "    log.basicConfig(level=log.INFO,\n",
    "                    format=\"%(asctime)s %(name)-12s %(levelname)-8s %(message)s\",\n",
    "                    filename=logpath,\n",
    "                    filemode=\"w\" )\n",
    "    try:\n",
    "        job_id = os.environ['PBS_JOBID']\n",
    "        infer_file = os.path.join(args.out_dir,'i_progress_'+str(job_id)+'.txt')\n",
    "    except Exception as e:\n",
    "        log.warning(e)\n",
    "        log.warning(\"job_id: {}\".format(job_id))\n",
    "    \n",
    "    # Setup additional logging to console\n",
    "    console = log.StreamHandler()\n",
    "    console.setLevel(log.INFO)\n",
    "    formatter = log.Formatter(\"[ %(levelname)s ] %(message)s\")\n",
    "    console.setFormatter(formatter)\n",
    "    log.getLogger(\"\").addHandler(console)\n",
    "    \n",
    "    # Plugin initialization for specified device and load extensions library if specified\n",
    "    log.info(\"Initializing plugin for {} device...\".format(args.device))\n",
    "    ie = IECore()\n",
    "   \n",
    "    if args.cpu_extension and 'CPU' in args.device:\n",
    "        log.info(\"Loading plugins for {} device...\".format(args.device))\n",
    "        ie.add_extension(args.cpu_extension, \"CPU\")\n",
    "\n",
    "    # Read IR\n",
    "    log.info(\"Reading IR...\")\n",
    "    net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "\n",
    "    if \"CPU\" in args.device:\n",
    "        supported_layers = ie.query_network(net, \"CPU\")\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            log.warning(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(args.device, ', '.join(not_supported_layers)))\n",
    "            log.warning(\"Please try to specify cpu extensions library path in sample's command line parameters using -l \"\n",
    "                      \"or --cpu_extension command line argument\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "   \n",
    "    # Load network to the plugin\n",
    "    log.info(\"Loading model to the plugin\")\n",
    "    exec_net = ie.load_network(network=net, device_name=args.device)\n",
    "     \n",
    "    #CLASSES = 11\n",
    "    class_names = [\"8PSK\", \"AM-DSB\", \"AM-SSB\", \"BPSK\", \"CPFSK\", \"GFSK\", \"PAM4\", \"QAM16\", \"QAM64\", \"QPSK\", \"WBFM\"]\n",
    "\n",
    "    print(\"Preparing input blobs\")\n",
    "    \n",
    "    # We define the batch size as x for easier use throughout\n",
    "    \n",
    "    # Note that if you want to have a different batch size, you would have to create different IR (see the Juypter Notebook\n",
    "    # for more information on this)\n",
    "    x = 110\n",
    "    print(\"Batch size is {}\".format(x))\n",
    "        \n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    total_inference = 0\n",
    "    topnum = 0\n",
    "    j = 0\n",
    "    \n",
    "    def run_it(start):\n",
    "        #Setup an array to run inference on\n",
    "        modulations = np.ndarray(shape=(x, 128, 1, 2))\n",
    "        \n",
    "        #Fill up the input array for this batch\n",
    "        stop = start + x\n",
    "        i = 0\n",
    "        \n",
    "        for item in X[start:stop]:\n",
    "            modulations[i] = item.reshape([1,2,128]).transpose(2, 0, 1)\n",
    "            i += 1\n",
    "\n",
    "        # Loading model to the plugin    \n",
    "        # Start inference\n",
    "        infer_time = []\n",
    "\n",
    "        t0 = time()\n",
    "        res = exec_net.infer(inputs={input_blob: modulations})\n",
    "        infer_time.append((time()-t0)*1000)\n",
    "\n",
    "        # Processing output blob\n",
    "        res = res[out_blob]\n",
    "        \n",
    "        #Check results for accuracy\n",
    "        \n",
    "        nonlocal correct\n",
    "        nonlocal wrong\n",
    "        nonlocal j\n",
    "        \n",
    "        # Function for calculating the amount correct, up to a certain \"num\"\n",
    "        # For example, for num = 3, would return topnum, representing the amount of \n",
    "        # times the correct label was one of the top 3 probabilities predicted\n",
    "        def topnum_accuracy(num):\n",
    "            nonlocal topnum\n",
    "            \n",
    "            for i in range(num):\n",
    "                det_label = class_names[top_ind[i]]\n",
    "\n",
    "                if det_label == labels[j][0]:\n",
    "                    topnum += 1\n",
    "                    return\n",
    "        \n",
    "        #Automatically calculates the accuracy for top 1 predictions\n",
    "        for i, probs in enumerate(res):\n",
    "            probs = np.squeeze(probs)\n",
    "            top_ind = np.argsort(probs)[-args.number_top:][::-1]\n",
    "           \n",
    "            det_label = class_names[top_ind[0]]\n",
    "            \n",
    "            if det_label == labels[j][0]:\n",
    "                correct = correct + 1\n",
    "            else:\n",
    "                wrong = wrong + 1        \n",
    "            \n",
    "            #Default to calculating top-3 accuracy\n",
    "            topnum_accuracy(3)\n",
    "            \n",
    "            j = j + 1        \n",
    "            \n",
    "        nonlocal total_inference\n",
    "        total_inference += np.sum(np.asarray(infer_time))\n",
    "        \n",
    "        \n",
    "    #Iterate through the whole dataset    \n",
    "    num_batches = X.shape[0]//x\n",
    "   \n",
    "    k = 0\n",
    "    print(\"Running inference: Batch 1\")\n",
    "    \n",
    "    #Run it on the dataset\n",
    "    for i in range(num_batches):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Running inference: Batch \" + str(i + 1))\n",
    "        run_it(k)\n",
    "        k += x\n",
    "         \n",
    "    # Print results    \n",
    "    print(\"Correct \" + str(correct))\n",
    "    print(\"Wrong \" + str(wrong))\n",
    "    print(\"Accuracy: \" + str(correct/(correct + wrong)))\n",
    "    print(\"Top \" + str(topnum) + \" Correct: \" + str(topnum))\n",
    "    print(\"Top \" + str(topnum) + \" Accuracy: \" + str(topnum/(correct + wrong)))\n",
    "\n",
    "    print(\"Average running time of one batch: {} ms\".format(total_inference/num_batches))\n",
    "    print(\"Total running time of inference: {} ms\" .format(total_inference))\n",
    "    print(\"Throughput: {} FPS\".format((1000*args.number_iter*x*num_batches)/total_inference))\n",
    "    \n",
    "    import platform\n",
    "    platform.processor()\n",
    "    print(\"Processor: \" + platform.processor())\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    #Cleanup\n",
    "    del net\n",
    "    del exec_net\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main() or 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Changing the Batch size\n",
    "\n",
    "In the above code, you may notice that the batch size is a constant \"x\", where x = 110. Changing this value will put errors into the code, because when the model was generated into IR, the \"input_shape\" paramater was \"[110, 1, 2, 128]\". If you wish to run the code with a different batch size, you will need to create new IR from the .onnx model, and specify the first value of the input_shape paramter to change the batch size. Then, in the code, you simply change x to the new value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the job file\n",
    "We will run inference on several different edge compute nodes present in the Intel® DevCloud for the Edge. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is a [Bash](https://www.gnu.org/software/bash/) script that serves as a wrapper around the Python* executable of our application that will be executed directly on the edge compute node.  One purpose of the job file is to simplify running an application on different compute nodes by accepting a few arguments and then performing accordingly any necessary steps before and after running the application executable.  \n",
    "\n",
    "For this sample, the job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "rf_classy.sh <output_directory> <device> <fp_precision> <input_file> <threshold>\n",
    "```\n",
    "Where the job file input arguments are:\n",
    "- <*output_directory*> - Output directory to use to store output files\n",
    "- <*device*> - Hardware device to use (e.g. CPU, GPU, etc.)\n",
    "- <*data_type*> - Which floating point precision inference model to use (FP32 or FP16)\n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "- Change to the working directory `PBS_O_WORKDIR` where this Jupyter* Notebook and other files appear on the compute node\n",
    "- Create the <*output_directory*>\n",
    "- Do additional setup and configuration when running inference on an FPGA hardware device\n",
    "- Choose the appropriate inference model IR file for the specified <*fp_precision*>\n",
    "- Run the application Python* executable with the appropriate command line arguments\n",
    "\n",
    "Run the following cell to create the `rf_classy.sh` job file.  The [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) line at the top will write the cell contents to the specified job file `rf_classy.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6feaa4a8de15>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6feaa4a8de15>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cd $HOME/rf\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Store input arguments: <output_directory> <device> <fp_precision> <input_file>\n",
    "cd $HOME/rf\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "DTYPE=$3\n",
    "\n",
    "# The default path for the job is the user's home directory,\n",
    "#  change directory to where the files are.\n",
    "\n",
    "# Make sure that the output directory exists.\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "# Check for special setup steps depending upon device to be used\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs - Updated for OpenVINO 2020.3\n",
    "    export AOCL_BOARD_PACKAGE_ROOT=/opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/BSP/a10_1150_sg2\n",
    "    source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh\n",
    "  #  export CL_CONTEXT_COMPILER_MODE_INTELFPGA=3\n",
    "  \n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/2020-4_PL2_FP11_AlexNet_GoogleNet_Generic.aocx\n",
    "\n",
    "fi\n",
    "\n",
    "# if [ \"$DTYPE\" = \"FP16\" ]; then\n",
    "#     export MODEL=$HOME/rf/FP16rfoink.xml\n",
    "    \n",
    "# else\n",
    "#     export MODEL=$HOME/rf/rfoink.xml\n",
    "    \n",
    "fi\n",
    "\n",
    "python3 $HOME/rf/rf_tutorial/large_rf.py -m $MODEL -d $DEVICE\n",
    "\n",
    "# Set inference model IR files using specified precision\n",
    "# Run the classifier  code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to submit a job\n",
    "\n",
    "Now that we have the job script, we can submit jobs to edge compute nodes in the Intel® DevCloud for the Edge.  To submit a job, the `qsub` command is used with the following format:\n",
    "```bash\n",
    "qsub <job_file> -N <JobName> -l <nodes> -F \"<job_file_arguments>\" \n",
    "```\n",
    "We can submit rd_classy.sh to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- <*job_file*> - This is the job file we created in the previous step\n",
    "- `-N` <*JobName*> : Sets name specific to the job so that it is easier to distinguish  between it and other jobs\n",
    "- `-l` <*nodes*> - Specifies the number and the type of nodes using the format *nodes*=<*node_count*>:<*property*>[:<*property*>...]\n",
    "- `-F` \"<*job_file_arguments*>\" - String containing the input arguments described in the previous step to use when running the job file\n",
    "\n",
    "*(Optional)*: To see the available types of nodes on the Intel® DevCloud for the Edge, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17 idc001skl,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe\r\n",
      "     16 idc002mx8,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "     13 idc003a10,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,hddl-f,iei-mustang-f100-a10\r\n",
      "     11 idc004nc2,compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,ram8gb,net1gbe,ncs,intel-ncs2\r\n",
      "      5 idc006kbl,compnode,iei,tank-870,intel-core,i5-7500t,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "      4 idc007xv5,compnode,iei,tank-870,intel-xeon,e3-1268l-v5,skylake,intel-hd-p530,ram32gb,net1gbe\r\n",
      "      7 idc008u2g,compnode,up-squared,grove,intel-atom,e3950,apollo-lake,intel-hd-505,ram4gb,net1gbe,ncs,intel-ncs2\r\n",
      "      1 idc009jkl,compnode,jwip,intel-core,i5-7500,kaby-lake,intel-hd-630,ram8gb,net1gbe\r\n",
      "      1 idc010jal,compnode,jwip,intel-celeron,j3355,apollo-lake,intel-hd-500,ram4gb,net1gbe\r\n",
      "      1 idc011ark2250s,compnode,advantech,intel-core,i5-6442eq,skylake,intel-hd-530,ram8gb,net1gbe\r\n",
      "      1 idc012ark1220l,compnode,advantech,intel-atom,e3940,apollo-lake,intel-hd-500,ram4gb,net1gbe\r\n",
      "      1 idc013ds580,compnode,advantech,intel-atom,e3950,apollo-lake,intel-hd-505,ram2gb,net1gbe\r\n",
      "     10 idc014upxa10fx1,compnode,aaeon,upx-edgei7,intel-core,i7-8665ue,whiskey-lake,intel-uhd-620,ram16gb,net1gbe,vpu,myriadx-ma2485\r\n",
      "      4 idc015ai5,compnode,advantech,epc-c301i5,intel-core,i5-8365ue,whiskey-lake,intel-uhd-620,ram8gb,net1gbe,vpu,myriadx-ma2485\r\n",
      "      3 idc016ai7,compnode,advantech,epc-c301i7,intel-core,i7-8665ue,whiskey-lake,intel-uhd-620,ram16gb,net1gbe,vpu,myriadx-ma2485\r\n",
      "      1 idc017,compnode,colfax,cx-e4150s-x7,intel-xeon,gold5220r,cascade-lake-r,ram96gb,net1gbe\r\n",
      "      1 idc018,compnode,colfax,cx-e4150s-x7,intel-xeon,gold6258r,cascade-lake-r,ram96gb,net1gbe\r\n",
      "      1 idc019,compnode,colfax,cx-e4150s-x7,intel-xeon,bronze3206r,cascade-lake-r,ram48gb,net1gbe\r\n",
      "      2 idc021,compnode,colfax,cx-e4150s-x7,intel-xeon,silver4214r,cascade-lake-r,ram48gb,net1gbe\r\n",
      "      2 idc023,compnode,advantech,epc-c301evk-s6a1,intel-core,i5-8365ue,whiskey-lake,ram8gb,net1gbe,vpu,myriad,x,ma2485\r\n",
      "      1 idc024,compnode,colfax,cx-e4150s-x7,intel-xeon,gold5220r,cascade-lake-r,ram96gb,net1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "      1 idc025,compnode,colfax,cx-e4150s-x7,intel-xeon,gold6258r,cascade-lake-r,ram96gb,net1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "      1 idc026,compnode,colfax,cx-e4150s-x7,intel-xeon,bronze3206r,cascade-lake-r,ram48gb,net1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "      1 idc027,compnode,colfax,cx-e4150s-x7,intel-xeon,silver4214r,cascade-lake-r,ram48gb,net1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "      2 idc028,compnode,iei,tank-870,intel-xeon,e3-1268l-v5,skylake,intel-hd-p530,ram32gb,net1gbe,hddl-f,iei-mustang-f100-a10\r\n",
      "      1 idc029,compnode,advantech,ax8665u-a1,intel-core,i7-8665u,whiskey,lake,intel-uhd-620,ram32gb,net1gbe\r\n",
      "      1 idc030,compnode,dell,intel-i7-10thgen,ram16gb,net1gbe\r\n"
     ]
    }
   ],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output from executing the previous cell, the properties describe the node, and the number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting a job on this notebook's node\n",
    "\n",
    "You can run the inference on the notebook the computer is connected to right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Initializing plugin for CPU device...\n",
      "[ INFO ] Reading IR...\n",
      "rf_classifier.py:123: DeprecationWarning: 'inputs' property of IENetwork class is deprecated. To access DataPtrs user need to use 'input_data' property of InputInfoPtr objects which can be accessed by 'input_info' property.\n",
      "  input_blob = next(iter(net.inputs))\n",
      "[ INFO ] Loading model to the plugin\n",
      "Preparing input blobs\n",
      "Batch size is 110\n",
      "Running inference: Batch 1\n",
      "Running inference: Batch 100\n",
      "Running inference: Batch 200\n",
      "Running inference: Batch 300\n",
      "Running inference: Batch 400\n",
      "Running inference: Batch 500\n",
      "Running inference: Batch 600\n",
      "Running inference: Batch 700\n",
      "Running inference: Batch 800\n",
      "Running inference: Batch 900\n",
      "Running inference: Batch 1000\n",
      "Running inference: Batch 1100\n",
      "Running inference: Batch 1200\n",
      "Running inference: Batch 1300\n",
      "Running inference: Batch 1400\n",
      "Running inference: Batch 1500\n",
      "Running inference: Batch 1600\n",
      "Running inference: Batch 1700\n",
      "Running inference: Batch 1800\n",
      "Running inference: Batch 1900\n",
      "Running inference: Batch 2000\n",
      "Correct 103642\n",
      "Wrong 116358\n",
      "Accuracy: 0.4711\n",
      "Top 3 Correct: 169792\n",
      "Top 3 Accuracy: 0.7717818181818182\n",
      "Average running time of one batch: 1.4426265954971313 ms\n",
      "Total running time of inference: 2885.2531909942627 ms\n",
      "Throughput: 76249.80736064541 FPS\n",
      "Processor: x86_64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 rf_classifier.py -m rf_FP16.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit jobs\n",
    "\n",
    "Each of the cells in the subsections below will submit a job to be run on different edge compute nodes. The output of each cell is the _JobID_ for the submitted job.  The _JobID_ can be used to track the status of the job.  After submission, a job will go into a waiting queue before running once the requested compute nodes become available.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>You may submit all jobs at once or one at a time.</i></div> \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>**Shift+Enter** will run the cell and automatically move you to the next cell. This allows you to use **Shift+Enter** multiple times to quickly run through multiple cells, including markdown cells.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Core™ i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html) processor. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44971.v-qsvr-1.devcloud-edge\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub rf_classy.sh -l nodes=1:i5-6500te -F \". CPU FP32\" -N rf_core\n",
    "print(job_id_core[0])\n",
    "#Progress indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Xeon® Processor E3-1268L v5](https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz.html). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41988.v-qsvr-1.devcloud-edge\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub rf_classy.sh  -l nodes=1:e3-1268l-v5 -F \". CPU FP32\" -N rf_xeon \n",
    "print(job_id_xeon[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Core CPU and using the integrated Intel® GPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html). The inference workload will run on the Intel® HD Graphics 530 GPU integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41994.v-qsvr-1.devcloud-edge\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub rf_classy.sh -l nodes=1:i5-6500te:intel-hd-530 -F \". GPU FP32\" -N rf_gpu \n",
    "print(job_id_gpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor job status\n",
    "\n",
    "To check the status of the jobs that have been submitted, use the `qstat` command.  The custom Jupyter* Notebook widget `liveQstat()` is provided to display the output of `qstat` with live updates.  \n",
    "\n",
    "Run the following cell to display the current job status with periodic updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liveQstat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-810ce089e239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mliveQstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'liveQstat' is not defined"
     ]
    }
   ],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs that you have submitted (referenced by the `JobID` that gets displayed right after you submit the jobs in the previous step).\n",
    "There should also be an extra job in the queue named `jupyterhub-singleuser`: this job is your current Jupyter* Notebook session which is always running.\n",
    "\n",
    "The `S` column shows the current status of each job: \n",
    "- If the status is `Q`, then the job is queued and waiting for available resources\n",
    "- If ste status is `R`, then the job is running\n",
    "- If the job is no longer listed, then the job has completed\n",
    "\n",
    "<br><div class=note><i><b>\n",
    "Note: The amount of time spent in the queue depends on the number of users accessing the requested compute nodes. Once the jobs for this sample application begin to run, they should take from 1 to 5 minutes each to complete.\n",
    "</b></i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.45px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
